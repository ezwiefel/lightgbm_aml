{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\n",
    "from azureml.pipeline.steps import EstimatorStep, PythonScriptStep, MpiStep, DatabricksStep\n",
    "from azureml.core.databricks import PyPiLibrary\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODES = 2\n",
    "LABEL_COLUMN_NAME = 'i_year1_renewal_flag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cluster = ws.compute_targets['amlcluster']\n",
    "databricks_cluster = ws.compute_targets['databricks']\n",
    "\n",
    "training_data = ws.datasets['renewal_train_csv']\n",
    "validation_data = ws.datasets['renewal_test_csv']\n",
    "\n",
    "lgbm_env = ws.environments['lightgbm-cli']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azureml/azureml_530807073dcffa9aa4f21724cf8481da'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_env.get_image_details(ws)['dockerImage']['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline Data to pass data from Coalesce -> Renaming steps\n",
    "db_saved_training_data = PipelineData('spark_csv_training', \n",
    "                                      datastore=ws.get_default_datastore(), \n",
    "                                      output_name='output_path')\n",
    "\n",
    "# Create Pipeline Data to pass data from Renaming Step -> LightGBM Step\n",
    "renamed_csv_train_data = PipelineData('renamed_csv_training', \n",
    "                                      datastore=ws.get_default_datastore(), \n",
    "                                      output_name='train_csv',\n",
    "                                      is_directory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#          COALESCE FILES           #\n",
    "#####################################\n",
    "# Spark is used to load any number  #\n",
    "# of CSV files and write them to    #\n",
    "# a specified number - passed as    #\n",
    "# '--number-of-files' parameter     #\n",
    "#####################################\n",
    "\n",
    "db_train_script_params = ['--number-of-files', str(NUM_NODES)]\n",
    "\n",
    "train_databricks_step = DatabricksStep(name='split_training_data', \n",
    "                                       spark_version='5.5.x-scala2.11',\n",
    "                                       inputs=[training_data.as_named_input('input_path').as_mount()], \n",
    "                                       outputs=[db_saved_training_data],\n",
    "                                       source_directory='../code/data_splitter',\n",
    "                                       python_script_params=db_train_script_params,\n",
    "                                       python_script_name='splitter.py',\n",
    "                                       num_workers=NUM_NODES,\n",
    "                                       node_type='Standard_DS4_v2',\n",
    "                                       compute_target=databricks_cluster,\n",
    "                                       pypi_libraries=[PyPiLibrary('click')],\n",
    "                                       allow_reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#          RENAMING FILES           #\n",
    "#####################################\n",
    "# Files are renamed from generic    #\n",
    "# Spark name to 'valid_[i].csv'     #\n",
    "# Where i will be used to choose    #\n",
    "# which MPI Rank will load the file #\n",
    "#####################################\n",
    "\n",
    "train_copy_files_params = ['--input_path', db_saved_training_data,\n",
    "                           '--output_path', renamed_csv_train_data,\n",
    "                           '--file_prefix', \"train\"]\n",
    "\n",
    "copy_train_files = PythonScriptStep(script_name='rename.py',\n",
    "                                    name='rename_training_data',\n",
    "                                    source_directory='../code/data_renamer',\n",
    "                                    inputs=[db_saved_training_data],\n",
    "                                    outputs=[renamed_csv_train_data],\n",
    "                                    compute_target=training_cluster,\n",
    "                                    arguments=train_copy_files_params,\n",
    "                                    allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline Data to pass data from Coalesce -> Renaming steps\n",
    "db_saved_validation_data = PipelineData('spark_csv_valid', \n",
    "                                      datastore=ws.get_default_datastore(), \n",
    "                                      output_name='output_path')\n",
    "\n",
    "# Create Pipeline Data to pass data from Renaming Step to LightGBM Step\n",
    "renamed_csv_valid_data = PipelineData('renamed_csv_validation', \n",
    "                                      datastore=ws.get_default_datastore(), \n",
    "                                      output_name='valid_csv',\n",
    "                                      is_directory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#          COALESCE FILES           #\n",
    "#####################################\n",
    "# Spark is used to load any number  #\n",
    "# of CSV files and write them to    #\n",
    "# a specified number - passed as    #\n",
    "# '--number-of-files' parameter     #\n",
    "#####################################\n",
    "\n",
    "db_valid_script_params = ['--number-of-files', str(NUM_NODES)]\n",
    "\n",
    "# Create a Databricks step that takes an arbitrary # of CSV files and writes them out to a specified number of files\n",
    "valid_databricks_step = DatabricksStep(name='split_validation_data', \n",
    "                                       spark_version='5.5.x-scala2.11',\n",
    "                                       inputs=[validation_data.as_named_input('input_path').as_mount()], \n",
    "                                       outputs=[db_saved_validation_data],\n",
    "                                       source_directory='../code/data_splitter',\n",
    "                                       python_script_params=db_valid_script_params,\n",
    "                                       python_script_name='splitter.py',\n",
    "                                       num_workers=NUM_NODES,\n",
    "                                       node_type='Standard_DS4_v2',\n",
    "                                       compute_target=databricks_cluster,\n",
    "                                       pypi_libraries=[PyPiLibrary('click')],\n",
    "                                       allow_reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#          RENAMING FILES           #\n",
    "#####################################\n",
    "# Files are renamed from generic    #\n",
    "# Spark name to 'valid_[i].csv'     #\n",
    "# Where i will be used to choose    #\n",
    "# which MPI Rank will load the file #\n",
    "#####################################\n",
    "\n",
    "valid_copy_files_params = ['--input_path', db_saved_validation_data,\n",
    "                           '--output_path', renamed_csv_valid_data,\n",
    "                           '--file_prefix', \"valid\"]\n",
    "\n",
    "copy_valid_files = PythonScriptStep(script_name='rename.py',\n",
    "                                    name='rename_validation_data', \n",
    "                                    source_directory='../code/data_renamer',\n",
    "                                    inputs=[db_saved_validation_data],\n",
    "                                    outputs=[renamed_csv_valid_data],\n",
    "                                    compute_target=training_cluster, \n",
    "                                    arguments=valid_copy_files_params,\n",
    "                                    allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'distributed_backend' parameter will be deprecated. Please use 'distributed_training' instead.\n"
     ]
    }
   ],
   "source": [
    "lgbm_params = [\n",
    "    '--train_data', renamed_csv_train_data,\n",
    "    '--valid_data', renamed_csv_valid_data,\n",
    "    '--task', 'train',\n",
    "    '--conf_file', 'train.conf',\n",
    "    '--metric', 'auc,binary_logloss,binary_error,mean_absolute_error',\n",
    "    '--num_machines', NUM_NODES,\n",
    "    '--label_column', f\"name:{LABEL_COLUMN_NAME}\",\n",
    "    '--num_iterations', 100,\n",
    "    '--tree_learner', 'data'\n",
    "]\n",
    "\n",
    "from azureml.core import RunConfiguration\n",
    "\n",
    "mpi_run = MpiStep(name='distributed_lgbm', \n",
    "                  source_directory='../code/lightgbm/', \n",
    "                  script_name='train.py', \n",
    "                  arguments = lgbm_params, \n",
    "                  node_count=NUM_NODES, \n",
    "                  process_count_per_node=1, \n",
    "                  inputs=[renamed_csv_train_data, renamed_csv_valid_data],\n",
    "                  environment_definition=lgbm_env,\n",
    "                  compute_target=training_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(ws, [mpi_run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step distributed_lgbm is ready to be created [27673a03]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step distributed_lgbm [27673a03][cefc0c20-ebde-4615-934e-69a4cc10d3c5], (This step will run and generate new outputs)\n",
      "Created step rename_training_data [9fbe37df][d1834bb7-7f9f-40fb-944a-8d918ed30ae8], (This step is eligible to reuse a previous run's output)\n",
      "Created step split_training_data [cdf7765e][a7421e61-184e-4d82-bb74-5ce474b4898e], (This step is eligible to reuse a previous run's output)\n",
      "Created step rename_validation_data [0c99ff97][cc2ce07d-1512-42d9-9457-48992af6a921], (This step is eligible to reuse a previous run's output)\n",
      "Created step split_validation_data [031f0a27][b1deb162-b869-41b7-a6c4-feaad4d0d074], (This step is eligible to reuse a previous run's output)\n",
      "Submitted PipelineRun 6ba3ab3e-05a4-477b-b595-2cabbbb581d7\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/test_datamunging/runs/6ba3ab3e-05a4-477b-b595-2cabbbb581d7?wsid=/subscriptions/dcdc374c-3ce4-4e43-92ad-10070b3b2941/resourcegroups/smart-assistant-ds/workspaces/smart-assistant-ws\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>test_datamunging</td><td>6ba3ab3e-05a4-477b-b595-2cabbbb581d7</td><td>azureml.PipelineRun</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/experiments/test_datamunging/runs/6ba3ab3e-05a4-477b-b595-2cabbbb581d7?wsid=/subscriptions/dcdc374c-3ce4-4e43-92ad-10070b3b2941/resourcegroups/smart-assistant-ds/workspaces/smart-assistant-ws\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: test_datamunging,\n",
       "Id: 6ba3ab3e-05a4-477b-b595-2cabbbb581d7,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: NotStarted)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.submit('test_datamunging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
